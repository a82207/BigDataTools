{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c821681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "# This example uses a local cluster, you can modify master to use  YARN or K8S if available \n",
    "# This example downloads sparkMeasure 0.14 for scala 2_11 from maven central\n",
    "\n",
    "spark = SparkSession \\\n",
    " .builder \\\n",
    " .master(\"local[*]\") \\\n",
    " .appName(\"Test sparkmeasure instrumentation of Python/PySpark code\") \\\n",
    " .config(\"spark.jars.packages\",\"ch.cern.sparkmeasure:spark-measure_2.12:0.17\")  \\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbb27a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|    Greeting|\n",
      "+---+------------+\n",
      "|  1|Hello world!|\n",
      "+---+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# test that Spark is working OK\n",
    "spark.sql(\"select 1 as id, 'Hello world!' as Greeting\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "434b582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Python wrapper API for spark-measure\n",
    "\n",
    "# !pip install sparkmeasure\n",
    "\n",
    "# !pyspark --packages ch.cern.sparkmeasure:spark-measure_2.12:0.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff526d68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the Python API in sparkmeasure package\n",
    "# an attache the sparkMeasure Listener for stagemetrics to the active Spark session\n",
    "\n",
    "from sparkmeasure import StageMetrics\n",
    "stagemetrics = StageMetrics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e517ce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/09 17:06:57 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "21/07/09 17:06:57 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 4\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 2\n",
      "numTasks => 19\n",
      "elapsedTime => 76552 (1.3 min)\n",
      "stageDuration => 75957 (1.3 min)\n",
      "executorRunTime => 285541 (4.8 min)\n",
      "executorCpuTime => 185704 (3.1 min)\n",
      "executorDeserializeTime => 998 (1.0 s)\n",
      "executorDeserializeCpuTime => 422 (0.4 s)\n",
      "resultSerializationTime => 7 (7 ms)\n",
      "jvmGCTime => 3784 (4 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 0 (0 ms)\n",
      "resultSize => 29437 (28.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 0\n",
      "recordsRead => 5000001\n",
      "bytesRead => 2391457208 (2.0 GB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 0\n",
      "shuffleTotalBlocksFetched => 0\n",
      "shuffleLocalBlocksFetched => 0\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 0 (0 Bytes)\n",
      "shuffleLocalBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsWritten => 0\n"
     ]
    }
   ],
   "source": [
    "stagemetrics.begin()\n",
    "\n",
    "df2 = spark.read.csv(\"SUSY.csv\", header=False, inferSchema = True)\n",
    "\n",
    "stagemetrics.end()\n",
    "\n",
    "# Print a summary report\n",
    "stagemetrics.print_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6910d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/09 17:08:11 WARN StageMetrics: Accumulables metrics data refreshed into temp view AccumulablesStageMetrics\n",
      "[Stage 13:===================================================>  (192 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregated Spark accumulables of type internal.metric. Sum of values grouped by metric name\n",
      "Name => sum(value) [group by name]\n",
      "\n",
      "executorCpuTime => 185704 (3.1 min)\n",
      "executorDeserializeCpuTime => 422 (0.4 s)\n",
      "executorDeserializeTime => 998 (1.0 s)\n",
      "executorRunTime => 285541 (4.8 min)\n",
      "input.bytesRead => 2391457208 (2.0 GB)\n",
      "input.recordsRead => 5000001\n",
      "jvmGCTime => 3784 (4 s)\n",
      "resultSerializationTime => 7 (7 ms)\n",
      "resultSize => 31383 (30.0 KB)\n",
      "\n",
      "SQL Metrics and other non-internal metrics. Values grouped per accumulatorId and metric name.\n",
      "Accid, Name => max(value) [group by accId, name]\n",
      "\n",
      "   39, number of output rows => 1\n",
      "   40, number of output rows => 1\n",
      "   69, number of output rows => 5000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print additional metrics from accumulables\n",
    "stagemetrics.print_accumulables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a836b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
